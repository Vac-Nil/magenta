import time
import os
import sys
import copy

from magenta import music as mm
from magenta.models.music_vae import configs
from magenta.models.music_vae import TrainedModel
from magenta.protobuf import music_pb2
import numpy as np 
import tensorflow as tf

flags = tf.app.flags
logging = tf.logging
FLAGS = flags.FLAGS

flags.DEFINE_string(
    'run_dir', None,
    'Path to the directory where the latest checkpoint will be loaded from.')
flags.DEFINE_string(
    'checkpoint_file', None,
    'Path to the checkpoint file. run_dir will take priority over this flag.')
flags.DEFINE_string(
    'output_dir', '/tmp/music_vae/generated',
    'The directory where MIDI files will be saved to.')
flags.DEFINE_string(
    'config', None,
    'The name of the config to use.')
flags.DEFINE_string(
    'config_midime', None,
    'The configuration for midime training model')
flags.DEFINE_string(
    'example_midi_dir', None,
    'Path of start MIDI file for personalization.')
flags.DEFINE_integer(
    'num_outputs', 5,
    'The number of samples to produce.')
flags.DEFINE_integer(
    'max_batch_size', 8,
    'The maximum batch size to use. Decrease if you are seeing an OOM.')
flags.DEFINE_float(
    'temperature', 0.5,
    'The randomness of the decoding process.')
flags.DEFINE_string(
    'log', 'INFO',
    'The threshold for what messages will be logged: '
    'DEBUG, INFO, WARN, ERROR, or FATAL.')

'''
Class for sampling from a multivariate Gaussian distribution.
'''
class SamplingLayer(tf.keras.layers.Layer):
    def __init__(self, num_outputs):
        super(SamplingLayer, self).__init__()
        self.num_outputs = num_outputs
    
    def call(self, inputs):
        mu, sigma = inputs
        return tf.add(tf.multiply(tf.random.normal([1, self.num_outputs]), sigma), mu)
    
'''
An interface for providing configurable properties to the MidiMe model.
@param input_size The size of the VAE input. Since the inputs to this
VAE are actually latent vectors from MusicVAE, then this number should be
equal to the number of latent variables used by MusicVAE (`zDims`). The
default is 256.
@param latent_size The size of the model's latent vector. The default is 4.
@param encoder_layers The shape of the layers in the Encoder network. The
default is [1024, 256, 64].
@param decoder_layers The shape of the layers in the Decoder network. The
default is [64, 256, 1024].
@param beta Weight of the variational loss in the total VAE loss. Default
    is 1.
@param epochs Number of epochs to train for. Default is 10.
'''
MidiMeConfig = {'input_size':512, 'latent_size':4, 'encoder_layers':[1024, 256, 64], 
                'decoder_layers':[64, 256, 1024], 'beta':1, 'epochs':10, 
                'learningRate': 0.001}
    
'''
Main `MidiMe` model class.

A `MidiMe` is a hierarchical variational autoencoder that is trained on
latent vectors generated by `MusicVAE`. It allows you to personalize your own
MusicVAE model with just a little data, so that samples from MidiMe sound
similar to the input data.
'''
class MidiMe:
    #`MidiMe` constructor.
    #@param config (optional) Model configuration.
    def __init__(self, config):
        self.config = config
        self.initialized = False
        self.samplinglayer = SamplingLayer(self.config['latent_size'])
        
    #Instantiates the `Encoder`, `Decoder` and the main `VAE`.
    def initialize(self):    
        x = tf.keras.Input(shape=(self.config['input_size'],))
        
        #Encoder model, goes from the original input, returns an output.
        self.encoder = self.__getEncoder(x)
        z, _, _ = self.encoder(x)
        
        #Decoder model, goes from the output of the encoder, to the final output.
        self.decoder = self.__getDecoder(z.shape[1])
        y = self.decoder(z)
        
        self.vae = tf.keras.Model(inputs=x, outputs=y, name='vae')
        
        self.initialized = True
        logging.info('Initialized model', 'MidiMe')
    
    '''
    Trains the `VAE` on the provided data. The number of epochs to train for
    is taken from the model's configuration.
    @param callback A function to be called at the end of every training epoch, 
    containing the training errors for that epoch.
    '''
    def train(self, xTrain, callback=False):        
        optimizer = tf.keras.optimizers.Adam(self.config['learningRate'])
        
        if(callback):
            pass
        
        else:
            self.vae.compile(optimizer, self.__loss)
            self.vae.fit(xTrain, xTrain, epochs=self.config['epochs'])
            
        logging.info('Training finished', 'MidiMe')
            
    '''
    Samples sequences from the model prior.
   
    @param numSamples The number of samples to return.
    @returns A latent vector representing a `NoteSequence`. You can pass
    this latent vector to a `MusicVAE`s `decode` method to convert it to a
    `NoteSequence`.
    '''
    def sample(self, numSamples=1):
        if(not self.initialized):
            self.initialize()
            
        randZs = tf.random.normal([numSamples, self.config['latent_size']])
        return tf.keras.backend.eval(self.decoder(randZs))
    
    '''
    Decodes a batch of latent vectors.
   
    @param z The batch of latent vectors, of shape `[numSamples,
        this.config['latent_size']]`.
    @returns A latent vector representing a `NoteSequence`. You can pass
    this latent vector to a `MusicVAE`s `decode` method to convert it to a
    `NoteSequence`.
    '''
    def decode(self, z):
        if(not self.initialized):
            self.initialize()
            
        return self.decoder.predict(z)
    
    '''
    Encodes a batch of latent vectors.
   
    @param z The batch of latent vectors, of shape `[numSamples,
    this.config['input_size']]`. This is the vector that you would get from
    passing a `NoteSequence` to a `MusicVAE`s `encode` method.
    @returns A latent vector of size this.config['latent_size'].
    '''
    def encode(self, z):
        if(not self.initialized):
            self.initialize()
        
        z_, _, _ = self.encoder.predict(z)
        return z_
    
    '''
    Reconstructs an input latent vector.
   
    @param z The input latent vector
    @returns The reconstructed latent vector after running it through the
        model.
    '''
    def predict(self, z):
        return self.vae.predict(z)
    
    def __getEncoder(self, input):
        x = input
        
        for i in self.config['encoder_layers']:
            x = tf.keras.layers.Dense(units=i, activation='relu')(x)
        
        mu = self.getAffineLayers(x, self.config['latent_size'], False)
        
        sigma = self.getAffineLayers(x, self.config['latent_size'], True)
        
        z = self.samplinglayer([mu, sigma])
        
        return tf.keras.Model(inputs=input, outputs=[z, mu, sigma], name='encoder')
    
    def __getDecoder(self, shape):
        z = tf.keras.Input(shape=(shape,))
        x = z
        
        for i in self.config['decoder_layers']:
            x = tf.keras.layers.Dense(units=i, activation='relu')(x)
            
        mu = self.getAffineLayers(x, self.config['input_size'], False)
        return tf.keras.Model(inputs=z, outputs=mu, name='decoder')
    
    def __loss(self, yTrue, yPred):
        _, zMu, zSigma = self.encoder(yTrue)
        
        #How closely the z matches a unit gaussian.
        latentLoss = self.klLoss(zMu, zSigma)
        
        #How well we regenerated yTrue.
        reconLoss = self.reconstructionLoss(yTrue, yPred)
        
        totalLoss = latentLoss*self.config['beta'] + reconLoss
        
        return totalLoss
    
    def reconstructionLoss(self, yTrue, yPred):
        # = mse(x,p_x_mu) / 2 'input_sigma ^2
        se = tf.keras.backend.pow(tf.keras.layers.subtract([yTrue, yPred]), 2)
        nll = tf.keras.layers.Lambda(lambda x: x/tf.keras.backend.pow(tf.keras.backend.ones((1)), 2)*2)(se)
        return tf.keras.backend.mean(tf.keras.backend.sum(nll, -1))
    
    def klLoss(self, mu, sigma):
        mu2 = tf.keras.backend.pow(mu, 2)
        sigma2 = tf.keras.backend.pow(sigma, 2)
        
        term1 = tf.keras.layers.Lambda(lambda x: tf.keras.backend.log(x)+1)(sigma2)
        term2 = tf.keras.layers.add([mu2, sigma2])
        term = tf.keras.layers.subtract([term1, term2])
        mean = tf.keras.backend.mean(tf.keras.backend.sum(term, -1))
        return tf.keras.layers.Lambda(lambda x: x *-0.5)(mean)
    
    def getAffineLayers(self, x, outputSize, softplus):
        output = tf.keras.layers.Dense(units=outputSize)(x)
        
        if(softplus):
            return tf.keras.layers.Activation('softplus')(output)
        else:
            return output
            
def train_model(xTrain, config):
    model = MidiMe(config)
    model.initialize()
    model.train(xTrain)
    return model

def trimSilence(ns):
    for i in ns:
        notes = sorted(
            list(i.notes), key=lambda note: note.start_time)  
        silence = notes[0].start_time
        for j in i.notes:
            j.start_time -= silence
            j.end_time -= silence             

def getChunks(Mels, seq_len):
    chunks = []
    for i in Mels:
        time = seq_len/4/(i.tempos[0].qpm/60)
        melChunks = mm.sequences_lib.split_note_sequence(i, time)
        for k in melChunks:
            k.ClearField('subsequence_info')
            if k.total_time == time:
                chunks.append(k)
            
    return chunks

def update(midime_config, input):
    hparams = input.split(',')
    for i in hparams:
        hparam = i.split('=')
        if hparam[0] not in midime_config:
            raise ValueError('Midime does not have such hparam!')
        else:
            if isinstance(midime_config[hparam[0]], list):
                midime_config[hparam[0]] = list(hparam[1])
            elif isinstance(midime_config[hparam[0]], int):
                midime_config[hparam[0]] = int(hparam[1])
            elif isinstance(midime_config[hparam[0]], float):
                midime_config[hparam[0]] = float(hparam[1])
                

def run(config_map):
    '''
    Load model params, save config file and start trainer.

    Args:
        config_map: Dictionary mapping configuration name to Config object.

    Raises:
        ValueError: if required flags are missing or invalid.
    '''
    date_and_time = time.strftime('%Y-%m-%d_%H%M%S')
    
    if FLAGS.run_dir is None == FLAGS.checkpoint_file is None:
        raise ValueError(
            'Exactly one of `--run_dir` or `--checkpoint_file` must be specified.')
    if FLAGS.output_dir is None:
        raise ValueError('`--output_dir` is required.')
    tf.gfile.MakeDirs(FLAGS.output_dir)
    
    if FLAGS.example_midi_dir is None:
        raise ValueError(
            'example_midi_dir is required.')
    
    if FLAGS.config not in config_map:
        raise ValueError('Invalid config name: %s' % FLAGS.config)
    config = config_map[FLAGS.config]
    config.data_converter.max_tensors_per_item = None
    
    config_midime = MidiMeConfig
    if FLAGS.config_midime is not None:
        update(config_midime, FLAGS.config_midime)
            
    logging.info('Loading model...')
    if FLAGS.run_dir:
        checkpoint_dir_or_path = os.path.expanduser(
            os.path.join(FLAGS.run_dir, 'train'))
    else:
        checkpoint_dir_or_path = os.path.expanduser(FLAGS.checkpoint_file)
    model = TrainedModel(
        config, batch_size=min(FLAGS.max_batch_size, FLAGS.num_outputs),
        checkpoint_dir_or_path=checkpoint_dir_or_path)
    
    example_sequences = []
    
    example_midi_dir = os.path.expanduser(FLAGS.example_midi_dir)
    files_in_dir = tf.gfile.ListDirectory(os.path.join(example_midi_dir))
    for file_in_dir in files_in_dir:
        full_file_path = os.path.join(example_midi_dir, file_in_dir)
        try: 
            example_sequences.append(mm.midi_file_to_note_sequence(full_file_path))
        except:
            raise ValueError('%s' %full_file_path)
    
    trimSilence(example_sequences)
    for i in example_sequences:
        i.tempos[0].time = 0
        del i.tempos[1:]
    
    chunks = getChunks(example_sequences, config.hparams.max_seq_len)
       
    latent = model.encode(chunks)[0]
    midime = train_model(latent, config_midime)
    
    s = midime.sample(FLAGS.num_outputs)
    samples = model.decode(s, config.hparams.max_seq_len)
    
    basename = os.path.join(
        FLAGS.output_dir,
        '%s_%s-*-of-%03d.mid' %
        (FLAGS.config, date_and_time, FLAGS.num_outputs))
    logging.info('Outputting %d files as `%s`...', FLAGS.num_outputs, basename)
    
    for i, ns in enumerate(samples):
        mm.sequence_proto_to_midi_file(ns, basename.replace('*', '%03d' % i))
        
    logging.info('Done.')

def main(unused_argv):
    logging.set_verbosity(FLAGS.log)
    run(configs.CONFIG_MAP)

def console_entry_point():
    tf.app.run(main)

if __name__ == '__main__':
    console_entry_point()